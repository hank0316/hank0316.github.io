---
layout: post
date: 2024-07-01
inline: true
related_posts: false
---

Our preprint [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](https://arxiv.org/abs/2407.01470) is out! We show that scalar reward models can be merged with intruction-tuned LLMs to derive domain-specific reward models w/o training! (Update at 2024/09/21: The paper is accepted to EMNLP 2024 Main!)