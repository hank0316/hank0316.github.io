---
---

@INPROCEEDINGS{10023234,
  author={Lin, Guan-Ting and Feng, Chi-Luen and Huang, Wei-Ping and Tseng, Yuan and Lin, Tzu-Han and Li, Chen-An and Lee, Hung-yi and Ward, Nigel G.},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={On the Utility of Self-Supervised Models for Prosody-Related Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1111},
  keywords={Analytical models;Conferences;Self-supervised learning;Data models;Data mining;Task analysis;Pragmatics;Speech Self-Supervised Learning;Representation Learning;Pretrained Models;Prosody;Pragmatics},
  doi={10.1109/SLT54892.2023.10023234}
}

@InProceedings{pmlr-v166-wu22a,
  title = 	 {The Ability of Self-Supervised Speech Models for Audio Representations},
  author =       {Wu, Tung-Yu and Hsu, Tsu-Yuan and Li, Chen-An and Lin, Tzu-Han and Lee, Hung-yi},
  booktitle = 	 {HEAR: Holistic Evaluation of Audio Representations (NeurIPS 2021 Competition)},
  pages = 	 {90--110},
  year = 	 {2022},
  editor = 	 {Turian, Joseph and Schuller, Björn W. and Herremans, Dorien and Kirchoff, Katrin and Perera, Paola Garcia and Esling, Philippe},
  volume = 	 {166},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--14 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v166/wu22a/wu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v166/wu22a.html},
  abstract = 	 {Self-supervised learning (SSL) speech models, which can serve as powerful upstream models to extract meaningful speech representations, have achieved unprecedented success in speech representation learning.  However, their effectiveness on non-speech datasets is relatively less explored. In this work, we propose an ensemble framework, with a combination of ensemble techniques, to fuse SSL speech models' embeddings. Extensive experiments on speech and non-speech audio datasets are conducted to investigate the representation abilities of our ensemble method and its single constituent model. Ablation studies are carried out to evaluate the performances of different ensemble techniques, such as feature averaging and concatenation. All experiments are conducted during NeurIPS 2021 HEAR Challenge as a standard evaluation pipeline provided by competition officials. Results demonstrate SSL speech models' strong abilities on various non-speech tasks, while we also note that they fail to deal with fine-grained music tasks, such as pitch classification and note onset detection. In addition, feature ensemble is shown to have great potential on producing more holistic representations, as our proposed framework generally surpasses state-of-the-art SSL speech/audio models and has superior performance on various datasets compared with other teams in HEAR Challenge. Our code is available at https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge—NTU-GURA.}
}

@inproceedings{hsueh-etal-2024-editing,
    title = "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
    author = "Hsueh, Cheng-Hsun  and
      Huang, Paul Kuo-Ming  and
      Lin, Tzu-Han  and
      Liao, Che Wei  and
      Fang, Hung-Chieh  and
      Huang, Chao-Wei  and
      Chen, Yun-Nung",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.550/",
    doi = "10.18653/v1/2024.findings-emnlp.550",
    pages = "9417--9429",
    abstract = "Knowledge editing is a rising technique for efficiently updating factual knowledge in large language models (LLMs) with minimal alteration of parameters. However, recent studies have identified side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. Despite these findings, evaluating the pitfalls of knowledge editing often relies on inconsistent metrics and benchmarks, lacking a uniform standard. In response, this survey presents a comprehensive study of these side effects, providing a unified perspective on the challenges of knowledge editing in LLMs by conducting experiments with consistent metrics and benchmarks. Additionally, we review related works and outline potential research directions to address these limitations. Our survey highlights the limitations of current knowledge editing methods, emphasizing the need for a deeper understanding of the inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials publicly (https://github.com/MiuLab/EditLLM-Survey)."
}

@inproceedings{lin-etal-2024-dogerm,
    title = "{D}oge{RM}: Equipping Reward Models with Domain Knowledge through Model Merging",
    author = "Lin, Tzu-Han  and
      Li, Chen-An  and
      Lee, Hung-yi  and
      Chen, Yun-Nung",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.868/",
    doi = "10.18653/v1/2024.emnlp-main.868",
    pages = "15506--15524",
    abstract = "Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the **Do**main knowled**ge** merged **R**eward **M**odel (**DogeRM**), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment."
}

@inproceedings{li-etal-2025-transferring,
    title = "Transferring Textual Preferences to Vision-Language Understanding through Model Merging",
    author = "Li, Chen-An  and
      Lin, Tzu-Han  and
      Chen, Yun-Nung  and
      Lee, Hung-yi",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-short.72/",
    doi = "10.18653/v1/2025.acl-short.72",
    pages = "923--943",
    ISBN = "979-8-89176-252-7",
    abstract = "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs."
}

@INPROCEEDINGS{10625957,
  author={Lin, Tzu-Han and Wang, How-Shing and Weng, Hao-Yung and Peng, Kuang-Chen and Chen, Zih-Ching and Lee, Hung-Yi},
  booktitle={2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={705-709},
  keywords={Adaptation models;Conferences;Merging;Speech recognition;Self-supervised learning;Speech enhancement;Signal processing;Parameter-efficient learning;adapters;network architecture search;ensemble learning},
  doi={10.1109/ICASSPW62465.2024.10625957}
}

@article{li2025silence,
  title={When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models},
  author={Li, Chen-An and Lin, Tzu-Han and Lee, Hung-yi},
  journal={arXiv preprint arXiv:2510.00626},
  year={2025}
}